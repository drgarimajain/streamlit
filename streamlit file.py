# -*- coding: utf-8 -*-
"""A2_23D1269.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lo93qOXL4qENqL8GyX_kV-s2N8mOLhiT
"""

import pandas as pd

import seaborn as sns

red = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/wine+quality/winequality-red.csv' , delimiter=';')

print(red.columns.tolist())

# Show the first 5 rows of the dataset
print(red.head())

# Code taken from  stack overfolw

white = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/wine+quality/winequality-white.csv', delimiter=';')



column_means = red.mean()

# Printing mean of each column with its name
print("Mean of each numeric column:")
for column_name, mean_value in column_means.items():
    print(f"{column_name}: {mean_value}")
# code taken from chatgpt
# mean of each column of redwine taken

column_means = white.mean()

# Printing mean of each column with its name
print("Mean of each numeric column:")
for column_name, mean_value in column_means.items():
    print(f"{column_name}: {mean_value}")

red_means = red.mean(numeric_only=True)

mean_values_comparisonred = pd.DataFrame({'Red wine': red_means })
# Plotting
mean_values_comparisonred.plot(kind='bar', figsize=(5, 5))
plt.title('Mean Values of Characteristics: Red Wine')
plt.xlabel('Characteristics')
plt.ylabel('Mean Value')
plt.legend(title='Wine Type')
plt.show()

#label a segment of data
qualitycorrelation_data = red[['quality','alcohol','residual sugar','volatile acidity','sulphates']]

# Calculating the correlation between datasets
corr_matrix = qualitycorrelation_data.corr()

# Visualize the correlation matrix
plt.figure(figsize=(5,5))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=.4, fmt=".4f")
plt.title('Correlation of "Quality of Red wine" with different physiochemical properties')  # Chart title
plt.show()  # Show the plot

#preprocessing
#droppingMissing values
#code taken from sci-kit learn libraray
#importing the sci-kit learn library
from sklearn.model_selection import train_test_split

red = red.dropna()

#splitting the data into training and testing dataset
X = red.drop('quality', axis=1)  # Feature to be predicted
y = red['quality']  # defining the target variable

# splitting the data into train and test set
# taking 25% of data for testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=25)

#since the unit of measurement of different columns varies, in order to standardise the scales I have used the code standardscaleer from scikit libraray
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

# Fit on training data
X_train_scaled = scaler.fit_transform(X_train)

# Only transform the test data
X_test_scaled = scaler.transform(X_test)

#code taken from Scikit-learn's preprocessing module

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error
from keras.models import Sequential
from keras.layers import Dense
import pandas as pd
import numpy as np



rf_parameters = {'n_estimators': [250, 500], 'max_depth': [25, 50, None]}
rf_grid_search = GridSearchCV(RandomForestRegressor(random_state=25), rf_parameters, cv=10)
rf_grid_search.fit(X_train, y_train)
best_rf_model = rf_grid_search.best_estimator_
rf_predictions = best_rf_model.predict(X_test)
rf_mse = mean_squared_error(y_test, rf_predictions)



#  print  Mean squared error
print('Random Forest MSE:', rf_mse)

from sklearn.svm import SVR
svr_params = {'C': [0.1, 1, 10], 'gamma': [0.01, 0.1, 1]}
svr_grid_search = GridSearchCV(SVR(kernel='rbf'), svr_params, cv=5)
svr_grid_search.fit(X_train, y_train)
best_svr_model = svr_grid_search.best_estimator_
svr_predictions = best_svr_model.predict(X_test)
svr_mse = mean_squared_error(y_test, svr_predictions)
#  print  MSE for
print('SVM MSE:', svr_mse)

importances = best_rf_model.feature_importances_
import matplotlib.pyplot as plt

features = X_train.columns
indices = np.argsort(importances)

plt.title('Feature Importances in Random Forest')
plt.barh(range(len(indices)), importances[indices], color='b', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()

from sklearn.inspection import permutation_importance

# Train the SVM model
svr = SVR(kernel='rbf', C=1, gamma=0.1)
svr.fit(X_train, y_train)

# Compute Permutation Feature Importance
perm_importance = permutation_importance(svr, X_test, y_test, n_repeats=30, random_state=42)

# Plotting
sorted_idx = perm_importance.importances_mean.argsort()
plt.figure(figsize=(4, 4))
plt.barh(range(X_train.shape[1]), perm_importance.importances_mean[sorted_idx])
plt.yticks(range(X_train.shape[1]), X_train.columns[sorted_idx])
plt.title("Permutation Importance with SVM")
plt.show()

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# Assume red_wine_data and white_wine_data are your preprocessed datasets for red and white wines, respectively
# Also assume that they both have the same features and a target variable 'quality'

# Split the datasets
X_red = red.drop('quality', axis=1)
y_red = red['quality']
X_white = white.drop('quality', axis=1)
y_white = white['quality']

# Train a model for red wine
red_model = RandomForestRegressor()
red_model.fit(X_red, y_red)

# Train a model for white wine
white_model = RandomForestRegressor()
white_model.fit(X_white, y_white)

# Test the red model with white wine data
red_predictions_on_white = red_model.predict(X_white)
mse_red_on_white = mean_squared_error(y_white, red_predictions_on_white)

# Test the white model with red wine data
white_predictions_on_red = white_model.predict(X_red)
mse_white_on_red = mean_squared_error(y_red, white_predictions_on_red)

# Print the MSE for comparison
print("MSE of red wine model on white wine data:", mse_red_on_white)
print("MSE of white wine model on red wine data:", mse_white_on_red)

# Question 2 Down syndrome
Mousedata = pd.read_excel('/content/drive/MyDrive/Data_Cortex_Nuclear.xls')

print(Mousedata.columns)

miceinfor = Mousedata.info()

# counting the number of Controls and cases in the total datset

genotype_counts = Mousedata['Genotype'].value_counts()

print(genotype_counts)

# mean of each varaible in Control vs case

mean_by_genotype = Mousedata.groupby('Genotype').mean()

print(mean_by_genotype)

#viewing the plot


# Plot
mean_by_genotype.plot(kind='bar', figsize=(30, 20))

plt.title('Mean Values of Each Feature by Genotype')
plt.xlabel('Feature')
plt.ylabel('Mean Value')
plt.legend(title='Genotype')
plt.show()

# Calculate the max for each column by genotype
max_by_genotype = Mousedata.groupby('Genotype').max()

# Calculate the min for each column by genotype
min_by_genotype = Mousedata.groupby('Genotype').min()

# Print the maximum values for each genotype
print("Maximum values by genotype:")
print(max_by_genotype)

# Print the minimum values for each genotype
print("\nMinimum values by genotype:")
print(min_by_genotype)

print(Mousedata.dtypes)

#Hilight the numeric data vs categorical data
is_numeric = Mousedata.select_dtypes(include=['int64', 'float64']).columns

numeric_data = Mousedata[is_numeric]
categorical_data = Mousedata.select_dtypes(exclude=['int64', 'float64'])

from sklearn import preprocessing
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import SimpleImputer
from sklearn.impute import IterativeImputer
from sklearn.linear_model import BayesianRidge
import pandas as pd
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.ensemble import RandomForestRegressor

# Assuming df is your DataFrame and it has numerical columns with missing values
simple_imputer = SimpleImputer(strategy='mean')
numeric_data_preimputed = simple_imputer.fit_transform(numeric_data)
imputer = IterativeImputer(estimator=RandomForestRegressor(n_estimators=50, n_jobs=-1), max_iter=10, tol=1e-3, verbose=2)
# Fit on the dataset and transform it to impute the missing values

imputed_data = imputer.fit_transform(numeric_data)
# Converting the imputed numpy array back to a pandas DataFrame


imputed_numeric_data = pd.DataFrame(imputed_data, columns=numeric_data.columns)
# Now imputed_df has the missing values imputed, and you can use it for further analysis

NAs = pd.concat([imputed_numeric_data.isnull().sum()], axis=1, keys=["imputed_numeric_data"])
NAs[NAs.sum(axis=1)>0]

merged_Mousedata = pd.concat([imputed_numeric_data, categorical_data], axis=1)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Assuming 'Mousedata' is your DataFrame and 'Genotype' is the target variable.
# 'Treatment', 'Behavior', 'class', 'MouseID' are categorical features.

# Separate the target variable and features
X = merged_Mousedata.drop('Genotype', axis=1)  # Features
y = merged_Mousedata['Genotype']  # Target variable

# One-hot encode the categorical columns except the target variable
X_encoded = pd.get_dummies(X, columns=['Treatment', 'Behavior', 'class', 'MouseID'])

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.6, random_state=87)

# Initialize the RandomForestClassifier
clf = RandomForestClassifier(random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Predict on the test set
y_pred = clf.predict(X_test)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

numerical_features = merged_Mousedata.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_features = merged_Mousedata.select_dtypes(include=['object', 'category']).columns.tolist()
print("Numerical Features:", numerical_features)
print("Categorical Features:", categorical_features)

from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Define the numerical and categorical features
numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()

# Define your Column Transformer for preprocessing
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ])

# Initialize the SVM with RBF kernel
svm_model = SVC(kernel='rbf', random_state=42)

# Create the pipeline
svm_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', svm_model)
])

# Perform cross-validation
cv_scores = cross_val_score(svm_pipeline, X, y, cv=5, scoring='accuracy')

# Output the cross-validation results
print(cv_scores)
print("Average CV Score:", np.mean(cv_scores))
print("Standard Deviation of CV Scores:", np.std(cv_scores))

print(column_name)
merged_Mousedata.info()
merged_Mousedata.head()

from sklearn.feature_selection import RFE
from sklearn.tree import DecisionTreeClassifier
# input split
X_encoded = pd.get_dummies(merged_Mousedata.drop('Genotype', axis=1), columns=['Treatment', 'Behavior', 'class', 'MouseID'])

y = merged_Mousedata['Genotype']
print(y.dtype)

X.shape

y.shape

rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5)
rfe.fit(X_encoded, y)

for i, col in zip(range(X.shape[1]), X.columns):
  print(f"{col} selected={rfe.support_[i]} rank={rfe.ranking_[i]}")

#AMPKA_N and APP_N are the only 2 features with Rank 1

#Question 3
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
import torch.backends.cudnn as cudnn
import numpy as np
import torchvision
from torchvision import datasets, models, transforms
import matplotlib.pyplot as plt
import time
import os
from PIL import Image
from tempfile import TemporaryDirectory


cudnn.benchmark = True
plt.ion()   # interactive mode

data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}

data_dir = '/content/drive/MyDrive/hymenoptera_data'
import os
print(os.listdir(data_dir))
image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),
                                          data_transforms[x])
                  for x in ['train', 'val']}
dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,
                                             shuffle=True, num_workers=4)
              for x in ['train', 'val']}
dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}
class_names = image_datasets['train'].classes

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Set up different transformations for the training and validation sets
from os.path import join
from os import listdir
transformations = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(size=224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Resize(size=256),
        transforms.CenterCrop(size=224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ]),
}

data_path = '/content/drive/MyDrive/hymenoptera_data'
from os import listdir
print(listdir(data_path))
image_data = {section: datasets.ImageFolder(join(data_path, section),
                                            transformations[section])
              for section in ['train', 'val']}
data_loaders = {section: torch.utils.data.DataLoader(image_data[section], batch_size=4,
                                                     shuffle=True, num_workers=4)
                for section in ['train', 'val']}
data_volumes = {section: len(image_data[section]) for section in ['train', 'val']}
category_names = image_data['train'].classes

# Determine if a GPU is available and set the device accordingly
compute_device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

from os import listdir
print(listdir(data_path))
image_data = {section: datasets.ImageFolder(join(data_path, section),
                                            transformations[section])
              for section in ['train', 'val']}

def imshow(inp, title=None):
    """Display image for Tensor."""
    inp = inp.numpy().transpose((1, 2, 0))
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    inp = std * inp + mean
    inp = np.clip(inp, 0, 1)
    plt.imshow(inp)
    if title is not None:
        plt.title(title)
    plt.pause(0.001)  # pause a bit so that plots are updated


# Get a batch of training data
inputs, classes = next(iter(dataloaders['train']))

# Make a grid from batch
out = torchvision.utils.make_grid(inputs)

imshow(out, title=[class_names[x] for x in classes])

def train_model(model, criterion, optimizer, scheduler, num_epochs=25):
    since = time.time()

    # Create a temporary directory to save training checkpoints
    with TemporaryDirectory() as tempdir:
        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')

        torch.save(model.state_dict(), best_model_params_path)
        best_acc = 0.0

        for epoch in range(num_epochs):
            print(f'Epoch {epoch}/{num_epochs - 1}')
            print('-' * 10)

            # Each epoch has a training and validation phase
            for phase in ['train', 'val']:
                if phase == 'train':
                    model.train()  # Set model to training mode
                else:
                    model.eval()   # Set model to evaluate mode

                running_loss = 0.0
                running_corrects = 0

                # Iterate over data.
                for inputs, labels in dataloaders[phase]:
                    inputs = inputs.to(device)
                    labels = labels.to(device)

                    # zero the parameter gradients
                    optimizer.zero_grad()

                    # forward
                    # track history if only in train
                    with torch.set_grad_enabled(phase == 'train'):
                        outputs = model(inputs)
                        _, preds = torch.max(outputs, 1)
                        loss = criterion(outputs, labels)

                        # backward + optimize only if in training phase
                        if phase == 'train':
                            loss.backward()
                            optimizer.step()

                    # statistics
                    running_loss += loss.item() * inputs.size(0)
                    running_corrects += torch.sum(preds == labels.data)
                if phase == 'train':
                    scheduler.step()

                epoch_loss = running_loss / dataset_sizes[phase]
                epoch_acc = running_corrects.double() / dataset_sizes[phase]

                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')

                # deep copy the model
                if phase == 'val' and epoch_acc > best_acc:
                    best_acc = epoch_acc
                    torch.save(model.state_dict(), best_model_params_path)

            print()

        time_elapsed = time.time() - since
        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')
        print(f'Best val Acc: {best_acc:4f}')

        # load best model weights
        model.load_state_dict(torch.load(best_model_params_path))
    return model

def visualize_model(model, num_images=6):
    was_training = model.training
    model.eval()
    images_so_far = 0
    fig = plt.figure()

    with torch.no_grad():
        for i, (inputs, labels) in enumerate(dataloaders['val']):
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            for j in range(inputs.size()[0]):
                images_so_far += 1
                ax = plt.subplot(num_images//2, 2, images_so_far)
                ax.axis('off')
                ax.set_title(f'predicted: {class_names[preds[j]]}')
                imshow(inputs.cpu().data[j])

                if images_so_far == num_images:
                    model.train(mode=was_training)
                    return
        model.train(mode=was_training)

model_conv = torchvision.models.resnet18(weights='IMAGENET1K_V1')
for param in model_conv.parameters():
    param.requires_grad = False

# Parameters of newly constructed modules have requires_grad=True by default
num_ftrs = model_conv.fc.in_features
model_conv.fc = nn.Linear(num_ftrs, 2)

model_conv = model_conv.to(device)

criterion = nn.CrossEntropyLoss()

# Observe that only parameters of final layer are being optimized as
# opposed to before.
optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)

# Decay LR by a factor of 0.1 every 7 epochs
exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)

model_conv = train_model(model_conv, criterion, optimizer_conv,
                         exp_lr_scheduler, num_epochs=25)

visualize_model(model_conv)

plt.ioff()
plt.show()

"""# New Section"""



import joblib
import streamlit as smlt
joblib.dump(best_rf_model, 'random_forest_model.pkl')
loaded_model = joblib.load('random_forest_model.pkl')


smlt.title('Wine Quality Prediction App')
smlt.caption('Know the quality of Wine you drink!!!')
smlt.table(red)
smlt.table(white)

alcohol = smlt.slider('Alcohol percentage', min_value=8.0, max_value=100.0, value=0.5, step=0.5)
acidity = smlt.slider('Acidity', min_value=0.0, max_value=1.0, value=0.5, step=0.01)
citrus = smlt.slider('Citrus', min_value=0.0, max_value=1.0, value=0.5, step=0.01)


if smlt.button('Predict Quality'):
    # Put all feature inputs into a numpy array
    features = np.array([[acidity, citrus, quality]])  # Add the rest of the features here
    # Predict the quality
    quality = best_rf_model.predict(features)
    smlt.write(f'The predicted quality is: {quality[0]}')

